{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1886638e",
   "metadata": {},
   "source": [
    "## PointNet Training Module\n",
    "\n",
    "##### Mike Pieschl\n",
    "\n",
    "This module implements the original PointNet model described in PointNet: Deep Leanring on Point Sets for 3D Classification and Segmentation (2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be48c311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 10:46:03.999470: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'mat_ops' from '/mnt/c/AFIT/AAR/4. Code/point_cloud_analysis/mat_ops.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "import trimesh\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "import tf2onnx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import open3d as o3d\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import onnxruntime as ort\n",
    "import onnx\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from ipywidgets import interact\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "\n",
    "import importlib\n",
    "import PointNetSegmentation\n",
    "import PointCloudSet\n",
    "import mat_ops\n",
    "\n",
    "importlib.reload(PointNetSegmentation)\n",
    "importlib.reload(PointCloudSet)\n",
    "importlib.reload(mat_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1d0fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SAVED_DATA = True\n",
    "SAVE_DATA = True\n",
    "SHOW_DISPLAYS = True\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "PATIENCE = 50\n",
    "INPUT_SIZE = 4096\n",
    "LEARNING_RATE = 0.0001\n",
    "LR_DECAY_STEPS = 7000\n",
    "LR_DECAY_RATE = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a0282b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'models/'\n",
    "MESH_PATH = 'mesh/'\n",
    "FIGURE_PATH = 'figures/'\n",
    "DATA_PATH = 'data/'\n",
    "PALINDROME_DATA_PATH = '/mnt/c/repos/aburn/usr/hub/palindrome_playground/DataCollect/'\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "MODEL_NAME = '_test_pointnet_softmax'\n",
    "PC_NAME = 'collect_2025.Nov.12_16.57.13.0087094.UTC'\n",
    "\n",
    "class_labels = ['kc46']\n",
    "part_labels = ['fuselage', 'left_engine', 'right_engine', 'left_wing', 'right_wing', 'left_hstab', 'right_hstab', 'vstab', 'left_boom_stab', 'right_boom_stab', 'boom_wing', 'boom_hull', 'boom_hose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44665ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices( 'GPU' )\n",
    "if( len( physical_devices ) > 0 ): \n",
    "    print( 'GPUs Available: ', len( physical_devices ) )\n",
    "    tf.config.experimental.set_memory_growth( physical_devices[0], True )\n",
    "else:   print( \"No GPUs available.\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef828d6",
   "metadata": {},
   "source": [
    "#### Parse AftrBurner output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d35b69c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc = PointCloudSet.PointCloudSet(one_hot = True,\n",
    "#                                  class_labels = class_labels, \n",
    "#                                  part_labels = part_labels, \n",
    "#                                  pretrain_tnet = False, \n",
    "#                                  network_input_width = INPUT_SIZE,\n",
    "#                                  batch_size = 8,\n",
    "#                                  rand_seed = RANDOM_SEED)\n",
    "# pc.build_from_aftr_output(f'{PALINDROME_DATA_PATH}collect_2025.Nov.12_16.57.13.0087094.UTC')\n",
    "# pc.get_info()\n",
    "# with open(f'{DATA_PATH}collect_2025.Nov.12_16.57.13.0087094.UTC.pkl', 'wb') as p:\n",
    "#     pickle.dump(pc, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e76604",
   "metadata": {},
   "source": [
    "#### Load PointCloudSet using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c859d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = None\n",
    "with open(f'{DATA_PATH}{PC_NAME}.pkl', 'rb') as p:\n",
    "    pc = pickle.load(p)\n",
    "\n",
    "assert pc != None, 'PointCloudSet failed to load.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0190171",
   "metadata": {},
   "source": [
    "#### Define Training Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "298057e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_save_so3(point_cloud: PointCloudSet.PointCloudSet, name: str):    \n",
    "    so3_net = PointNetSegmentation.TNetRegressor(add_regularization = False)\n",
    "    so3_net.build(input_shape = (None, INPUT_SIZE, 3))\n",
    "\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        LEARNING_RATE,\n",
    "        decay_steps = LR_DECAY_STEPS,\n",
    "        decay_rate = LR_DECAY_RATE,\n",
    "        staircase = False\n",
    "    )\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(\n",
    "        learning_rate = lr_schedule\n",
    "    )\n",
    "\n",
    "    so3_net.compile(\n",
    "        optimizer = optimizer,\n",
    "        loss = 'mse',\n",
    "        metrics = [keras.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        patience = PATIENCE,\n",
    "        verbose = 1,\n",
    "        restore_best_weights = True\n",
    "    )\n",
    "\n",
    "    train = point_cloud.get_train_tnet_set()\n",
    "    val = point_cloud.get_val_tnet_set()\n",
    "\n",
    "    history = so3_net.fit(x = train, epochs = EPOCHS, verbose = 1, validation_data = val, callbacks = [early_stopping])\n",
    "    so3_net.input_transform.save_weights(f'{MODEL_PATH}{name}_tnet.weights.h5')\n",
    "\n",
    "    with open(f'{MODEL_PATH}{name}_tnet_history.json', 'w') as j:\n",
    "        json.dump({\n",
    "            'loss': history.history['loss'],\n",
    "            'val_loss': history.history['val_loss'],\n",
    "            'root_mean_squared_error': history.history['root_mean_squared_error'],\n",
    "            'val_root_mean_squared_error': history.history['val_root_mean_squared_error'],\n",
    "        }, j)\n",
    "\n",
    "    return so3_net\n",
    "\n",
    "def load_so3_net(name: str):\n",
    "    so3_net = PointNetSegmentation.TNetRegressor(add_regularization = False)\n",
    "    so3_net.build((None, INPUT_SIZE, 3))\n",
    "    so3_net.input_transform.load_weights(f'{MODEL_PATH}{name}_tnet.weights.h5', skip_mismatch = False)\n",
    "    return so3_net\n",
    "\n",
    "def train_save_pointnet_segmentater(point_cloud: PointCloudSet.PointCloudSet, name: str, use_pretrained_tnet: bool = False):    \n",
    "    model = PointNetSegmentation.PointNetSegmentation(output_width = len(part_labels))\n",
    "    model.build(input_shape = (None, INPUT_SIZE, 3))\n",
    "\n",
    "    if(use_pretrained_tnet):\n",
    "        model.input_transform.load_weights(f'{MODEL_PATH}{name}_tnet.weights.h5', skip_mismatch = False)\n",
    "        model.input_transform.trainable = False\n",
    "\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        LEARNING_RATE,\n",
    "        decay_steps = LR_DECAY_STEPS,\n",
    "        decay_rate = LR_DECAY_RATE,\n",
    "        staircase = False\n",
    "    )\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(\n",
    "        learning_rate = lr_schedule\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = optimizer,\n",
    "        loss = keras.losses.CategoricalCrossentropy(),\n",
    "        metrics = [keras.metrics.CategoricalAccuracy()]\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        patience = PATIENCE,\n",
    "        verbose = 1,\n",
    "        restore_best_weights = True\n",
    "    )\n",
    "\n",
    "    train = point_cloud.get_train_seg_set()\n",
    "    val = point_cloud.get_val_seg_set()\n",
    "\n",
    "    history = model.fit(x = train, epochs = EPOCHS, verbose = 1, validation_data = val, callbacks = [early_stopping])\n",
    "\n",
    "    model.save(f'{MODEL_PATH}{name}.keras')\n",
    "\n",
    "    with open(f'{MODEL_PATH}{name}_history.json', 'w') as j:\n",
    "        json.dump({\n",
    "            'loss': history.history['loss'],\n",
    "            'val_loss': history.history['val_loss'],\n",
    "            'categorical_accuracy': history.history['categorical_accuracy'],\n",
    "            'val_categorical_accuracy': history.history['val_categorical_accuracy'],\n",
    "        }, j)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcf3009",
   "metadata": {},
   "source": [
    "#### (Model40 Dataset for Verification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d8c3431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = tf.keras.utils.get_file(\"modelnet.zip\", \"http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\", extract = True)\n",
    "# DATA_DIR = os.path.join(os.path.dirname(DATA_DIR), \"modelnet_extracted/ModelNet10\")\n",
    "\n",
    "# def parse_dataset(num_points = INPUT_SIZE):\n",
    "\n",
    "#     train_points = []\n",
    "#     train_labels = []\n",
    "#     test_points = []\n",
    "#     test_labels = []\n",
    "#     class_map = {}\n",
    "#     folders = glob.glob(os.path.join(DATA_DIR, \"[!README]*\"))\n",
    "    \n",
    "#     for i, folder in tqdm(enumerate(folders)):\n",
    "#         print(f\"Processing class {os.path.basename(folder)}\")\n",
    "\n",
    "#         # Store folder name with ID so we can retrieve later\n",
    "#         class_map[i] = folder.split(\"/\")[-1]\n",
    "\n",
    "#         # Gath all files in folder\n",
    "#         train_files = glob.glob(os.path.join(folder, \"train/*\"))\n",
    "#         test_files = glob.glob(os.path.join(folder, \"test/*\"))\n",
    "\n",
    "#         for f in train_files:\n",
    "#             train_points.append(trimesh.load(f).sample(num_points))\n",
    "#             train_labels.append(i)\n",
    "\n",
    "#         for f in test_files:\n",
    "#             test_points.append(trimesh.load(f).sample(num_points))\n",
    "#             test_labels.append(i)\n",
    "    \n",
    "#     return (np.array(train_points),\n",
    "#             np.array(test_points),\n",
    "#             np.array(train_labels),\n",
    "#             np.array(test_labels),\n",
    "#             class_map)\n",
    "\n",
    "# def augment(points, label):\n",
    "#     # Jitter points\n",
    "#     points += tf.random.uniform(points.shape, -0.005, 0.005, dtype = tf.float64)\n",
    "    \n",
    "#     # Shuffle points\n",
    "#     points = tf.random.shuffle(points)\n",
    "\n",
    "#     return points, label\n",
    "\n",
    "# train_points, test_points, train_labels, test_labels, CLASS_MAP = parse_dataset(1024)\n",
    "\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((train_points, train_labels))\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((train_points, train_labels))\n",
    "\n",
    "# train_dataset = train_dataset.shuffle(len(train_points)).map(augment).batch(BATCH_SIZE)\n",
    "# test_dataset = test_dataset.shuffle(len(test_points)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c784b",
   "metadata": {},
   "source": [
    "#### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39671d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1763394439.047065   15312 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6078 MB memory:  -> device: 0, name: Quadro RTX 4000, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  obs = (6337, 4096, 3) | labels = (6337, 4096) \n",
      "Validation data size:  obs = (1268, 4096, 3) | labels = (1268, 4096) \n",
      "Epoch 1/3\n",
      "Training: True\n",
      "Training: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 10:49:35.679471: I external/local_xla/xla/service/service.cc:163] XLA service 0x75ee8000ba50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-11-17 10:49:35.679509: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): Quadro RTX 4000, Compute Capability 7.5\n",
      "2025-11-17 10:49:35.971130: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-11-17 10:49:37.604150: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91501\n",
      "2025-11-17 10:49:45.337297: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv (f32[1088,512,1,1]{3,2,1,0}, u8[0]{0}) custom-call(f32[1088,8,4096,1]{3,2,1,0}, f32[512,8,4096,1]{3,2,1,0}), window={size=4096x1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n",
      "2025-11-17 10:50:01.624056: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 17.286904025s\n",
      "Trying algorithm eng0{} for conv (f32[1088,512,1,1]{3,2,1,0}, u8[0]{0}) custom-call(f32[1088,8,4096,1]{3,2,1,0}, f32[512,8,4096,1]{3,2,1,0}), window={size=4096x1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false,\"reification_cost\":[]} is taking a while...\n",
      "I0000 00:00:1763394609.011480   15671 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m793/793\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - categorical_accuracy: 0.3651 - loss: 2.0678Training: False\n",
      "\u001b[1m793/793\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 160ms/step - categorical_accuracy: 0.3653 - loss: 2.0673 - val_categorical_accuracy: 0.5110 - val_loss: 3.1302\n",
      "Epoch 2/3\n",
      "\u001b[1m793/793\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 146ms/step - categorical_accuracy: 0.6460 - loss: 1.1450 - val_categorical_accuracy: 0.5921 - val_loss: 1.4326\n",
      "Epoch 3/3\n",
      "\u001b[1m793/793\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 157ms/step - categorical_accuracy: 0.7160 - loss: 0.9150 - val_categorical_accuracy: 0.6444 - val_loss: 1.2195\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 53ms/step - categorical_accuracy: 0.6333 - loss: 1.1714\n",
      "1.1164791584014893 0.640175998210907\n"
     ]
    }
   ],
   "source": [
    "if(pc != None):    \n",
    "    model = train_save_pointnet_segmentater(pc, f'{pc.get_description()}{MODEL_NAME}', use_pretrained_tnet = False)\n",
    "    loss, accuracy = model.evaluate(pc.get_test_seg_set())\n",
    "\n",
    "    print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa3d27a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/AFIT/AAR/4. Code/point_cloud_analysis/.venv/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 67 variables whereas the saved optimizer has 127 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You cannot add new elements of state (variables or sub-layers) to a layer that is already built. All state must be created in the `__init__()` method or in the `build()` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m\n\u001b[1;32m      8\u001b[0m test_reload_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     custom_objects \u001b[38;5;241m=\u001b[39m custom_objects\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m test_reload_model\u001b[38;5;241m.\u001b[39mcompile()\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtest_reload_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mINPUT_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m test_reload_model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m     17\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m test_reload_model\u001b[38;5;241m.\u001b[39mevaluate(pc\u001b[38;5;241m.\u001b[39mget_test_seg_set())\n",
      "File \u001b[0;32m/mnt/c/AFIT/AAR/4. Code/point_cloud_analysis/.venv/lib/python3.10/site-packages/keras/src/layers/layer.py:232\u001b[0m, in \u001b[0;36mLayer.__new__.<locals>.build_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_open_name_scope():\n\u001b[1;32m    231\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m current_path()\n\u001b[0;32m--> 232\u001b[0m     \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(original_build_method)\n",
      "File \u001b[0;32m/mnt/c/AFIT/AAR/4. Code/point_cloud_analysis/PointNetSegmentation.py:125\u001b[0m, in \u001b[0;36mPointNetSegmentation.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03mBuild the model by calling build on all sub-layers\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m@param input_shape: Expected input shape (b, n, 3)\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28msuper\u001b[39m(PointNetSegmentation, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mbuild(input_shape)\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_transform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_1_1\u001b[38;5;241m.\u001b[39mbuild((input_shape[\u001b[38;5;241m0\u001b[39m], input_shape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m, input_shape[\u001b[38;5;241m2\u001b[39m]))\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_1_2\u001b[38;5;241m.\u001b[39mbuild((input_shape[\u001b[38;5;241m0\u001b[39m], input_shape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m))\n",
      "File \u001b[0;32m/mnt/c/AFIT/AAR/4. Code/point_cloud_analysis/.venv/lib/python3.10/site-packages/keras/src/layers/layer.py:232\u001b[0m, in \u001b[0;36mLayer.__new__.<locals>.build_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_open_name_scope():\n\u001b[1;32m    231\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m current_path()\n\u001b[0;32m--> 232\u001b[0m     \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(original_build_method)\n",
      "File \u001b[0;32m/mnt/c/AFIT/AAR/4. Code/point_cloud_analysis/PointNetSegmentation.py:266\u001b[0m, in \u001b[0;36mTNet.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28msuper\u001b[39m(TNet, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mbuild(input_shape)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK \u001b[38;5;241m=\u001b[39m input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitializer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_initializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_weight(shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK), initializer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m'\u001b[39m, trainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/mnt/c/AFIT/AAR/4. Code/point_cloud_analysis/.venv/lib/python3.10/site-packages/keras/src/layers/layer.py:588\u001b[0m, in \u001b[0;36mLayer.add_weight\u001b[0;34m(self, shape, initializer, dtype, trainable, autocast, regularizer, constraint, aggregation, overwrite_with_gradient, name)\u001b[0m\n\u001b[1;32m    586\u001b[0m variable\u001b[38;5;241m.\u001b[39mconstraint \u001b[38;5;241m=\u001b[39m constraints\u001b[38;5;241m.\u001b[39mget(constraint)\n\u001b[1;32m    587\u001b[0m variable\u001b[38;5;241m.\u001b[39moverwrite_with_gradient \u001b[38;5;241m=\u001b[39m overwrite_with_gradient\n\u001b[0;32m--> 588\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_track_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m variable\n",
      "File \u001b[0;32m/mnt/c/AFIT/AAR/4. Code/point_cloud_analysis/.venv/lib/python3.10/site-packages/keras/src/layers/layer.py:1402\u001b[0m, in \u001b[0;36mLayer._track_variable\u001b[0;34m(self, variable)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_track_variable\u001b[39m(\u001b[38;5;28mself\u001b[39m, variable):\n\u001b[1;32m   1401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m variable\u001b[38;5;241m.\u001b[39mtrainable:\n\u001b[0;32m-> 1402\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_to_store\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrainable_variables\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1404\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tracker\u001b[38;5;241m.\u001b[39madd_to_store(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon_trainable_variables\u001b[39m\u001b[38;5;124m\"\u001b[39m, variable)\n",
      "File \u001b[0;32m/mnt/c/AFIT/AAR/4. Code/point_cloud_analysis/.venv/lib/python3.10/site-packages/keras/src/utils/tracking.py:119\u001b[0m, in \u001b[0;36mTracker.add_to_store\u001b[0;34m(self, store_name, value)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21madd_to_store\u001b[39m(\u001b[38;5;28mself\u001b[39m, store_name, value):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocked:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock_violation_msg)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[store_name][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstored_ids[store_name]\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(value))\n",
      "\u001b[0;31mValueError\u001b[0m: You cannot add new elements of state (variables or sub-layers) to a layer that is already built. All state must be created in the `__init__()` method or in the `build()` method."
     ]
    }
   ],
   "source": [
    "custom_objects = {\n",
    "    \"PointNetSegmentation\": PointNetSegmentation.PointNetSegmentation,\n",
    "    \"TNet\": PointNetSegmentation.TNet,\n",
    "    \"ConvLayer\": PointNetSegmentation.ConvLayer,\n",
    "    \"DenseLayer\": PointNetSegmentation.DenseLayer\n",
    "}\n",
    "\n",
    "test_reload_model = tf.keras.models.load_model(\n",
    "    f'{MODEL_PATH}{MODEL_NAME}.keras',\n",
    "    custom_objects = custom_objects\n",
    ")\n",
    "\n",
    "test_reload_model.compile()\n",
    "test_reload_model.build(input_shape = (None, INPUT_SIZE, 3))\n",
    "test_reload_model.summary()\n",
    "\n",
    "loss, accuracy = test_reload_model.evaluate(pc.get_test_seg_set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3266b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = np.expand_dims(pc.get_raw_test_set()['observations'][0], axis = 0)\n",
    "test_labels = pc.get_raw_test_set()['part_labels'][0]\n",
    "\n",
    "predict_labels = test_reload_model.predict(test_case).squeeze(axis = 0)\n",
    "part_labels_np = np.array(part_labels)\n",
    "print(f\"{np.sum(np.array([part_labels[i] == test_labels[i] for i in np.argmax(predict_labels, axis = -1)]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f99e8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_signature = [\n",
    "    tf.TensorSpec((None, INPUT_SIZE, 3), dtype = tf.float32)\n",
    "]\n",
    "\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(\n",
    "    test_reload_model,\n",
    "    input_signature = input_signature,\n",
    "    opset = 13\n",
    ")\n",
    "\n",
    "onnx.save(onnx_model, f'{MODEL_PATH}{MODEL_NAME}.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5434ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ONNX Runtime Providers: {ort.get_available_providers()}\")\n",
    "\n",
    "session = ort.InferenceSession(\n",
    "    f'{MODEL_PATH}{MODEL_NAME}.onnx',\n",
    "    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    ")\n",
    "\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "print(f'Input name: {input_name}')\n",
    "print(f'Output name: {output_name}')\n",
    "\n",
    "test_inputs_f32 = test_case.astype(np.float32)\n",
    "\n",
    "logits_output = session.run(\n",
    "    [output_name],\n",
    "    {input_name: test_inputs_f32}\n",
    ")\n",
    "\n",
    "print(logits_output)\n",
    "\n",
    "part_labels_np = np.array(part_labels)\n",
    "output_np = np.squeeze(np.array(logits_output), axis = (0, 1))\n",
    "print(f\"{np.sum(np.array([part_labels[i] == test_labels[i] for i in np.argmax(output_np, axis = -1)]))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
